<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<!-- <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" /> -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!-- <link rel="stylesheet" href="css/jemdoc.css" type="text/css" /> -->
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
<link rel='stylesheet' href="my_style.css">
<title>Yue's Homepage</title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-124162585-1");
    pageTracker._trackPageview();
} catch(err) {}</script>


<!--TODO THIS IS MY SCRIPT -->
<script>
function FadeIn(q) {
q.volume=0.05;
FadeInLoop(q);
}
function FadeInLoop(q){
  var end_vol=50;
  var vol = q.volume*100;
  if ( vol+1 < end_vol )
    {
        console.log(vol);
        q.volume = ((vol+1) / 100);
        setInterval(function() { FadeInLoop(q) }, 1200);
    }
}
</script>





<div class="sidenav">
  <div>
    <img class="clip-svg" src="data/avartar.jpg" width="180">
  </div>

  <div class="menu-title">INTRO</div>
  <div class="menu-item"><a href="#main_anchor">Home</a></div>
  <div class="menu-item"><a href="data/cv.pdf" target="_blank">CV <span class='fas fa-external-link-alt fa-1x'></span> </a></div>
  <div class="menu-item"><a href="#publications_anchor">Publications</a></div>

  <div class="menu-title">TECH</div>
  <div class="menu-item"><a href="#collecting_anchor" onclick="collapseFunction('collecting_anchor');">Collecting</a></div>
  <div class="menu-item"><a href="iros2018-slam-papers" target="_blank">IROS2018SLAM <span class='fas fa-external-link-alt fa-1x'></span> </a></div>

  <div class="menu-title">MISC</div>
  <div class="menu-item"><a href="https://www.bilibili.com/video/av35787189" target="_blank">我们自35班 <span class='fas fa-external-link-alt fa-1x'></span> </a></div>
  <div class="menu-item"><a href="#soccer_anchor" onclick="collapseFunction('soccer_anchor');">Soccer</a></div>
  <div class="menu-item"><a href="#running_anchor" onclick="collapseFunction('running_anchor');">Running</a></div>

  <div class="bottom-logos">
    <p></p>
    <a class="fa-a" href="https://www.facebook.com/yue.meng.75" target="_blank">
      <span class="fab fa-facebook fa-2x"></span>
    </a>
    <a class="fa-a" href="https://github.com/mengyuest" target="_blank">
      <span class="fab fa-github fa-2x"></span>
    </a>
    <a class="fa-a" href="https://www.linkedin.com/in/yuemeng95" target="_blank">
      <span class="fab fa-linkedin fa-2x"></span>
    </a>
    <a class="fa-a" href="https://www.instagram.com/misha_ucsd" target="_blank">
      <span class="fab fa-instagram fa-2x"></span>
    </a>
    <p></p>
    </div>

</div>
<div class="main" id="main_anchor">
  <h1>Meng, Yue (孟岳)</h1>
  <!-- <hr> </hr> -->
  <p style="color:#A51C30; font-weight:bold;">Looking for Research Scientist positions in Robotics / GenAI / LLM (United States, Fall 2025) <a href="data/cv.pdf" target="_blank">[CV]</a></p>
  <p>I am a 5<sup>th</sup> year PhD candidate at <a href="https://www.mit.edu/">MIT</a> <a href="https://aeroastro.mit.edu">AeroAstro</a>. I work in <a href="https://aeroastro.mit.edu/realm"><b>REALM lab</b></a>. <br />
    My research topic is <i>using deep learning techniques for robot planning and control tasks</i>. <br />
    Before that, I was an AI Resident at <a href="https://mitibmwatsonailab.mit.edu/?lnk=research">IBM Thomas J. Waston Research Center</a>.<br />
    I received an M.S. degree in <a href="https://www.ece.ucsd.edu/">ECE</a> at <a href="https://ucsd.edu/">UC San Diego</a> and worked in <a href="https://existentialrobotics.org/">ERL</a> and <a href="https://wcsng.ucsd.edu">WCSNG</a>, <br />
    and a B.S. degree in the <a href="https://tsinghua.edu.kg/publish/au/index.html">Department of Automation</a> from <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>.</p>



  <table >
  <tr class="row">
    <td class="logo-col">
      <a href="https://www.mit.edu/" target="_blank" title="MIT">
      <img class="logo-figure" src="data/logos/MIT-logo.png" alt="MIT"></a>
    </td>
    <td class="logo-col">
      <a href="https://ucsd.edu/" target="_blank" title="UCSD">
      <img class="logo-figure" src="data/logos/UCSD-logo.png" alt="UCSD"></a>
    </td>
    <td class="logo-col">
      <a href="https://www.tsinghua.edu.cn/en/" target="_blank" title="Tsinghua">
      <img class="logo-figure" src="data/logos/THU-logo.png" alt="Tsinghua"></a>
    </td>
    <td class="logo-col">
      <a href="http://www.tsinghuaveterans.com/" target="_blank" title="Tsinghua Veterans Soccer">
      <img class="logo-figure" src="data/logos/tsinghua-veteran-logo.png" alt="Tsinghua Veterans Soccer"></a>
    </td>
    <td class="logo-col">
      <a href="https://sites.google.com/view/sdred/" target="_blank" title="San Diego Red Army Soccer">
      <img class="logo-figure" src="data/logos/san-diego-red-army.png" alt="San Diego Red Army Soccer"></a>
    </td>
    <td class="logo-col">
      <a href="https://sites.google.com/view/nccsfweb/teams/calblue" target="_blank" title="Calblue Soccer Team">
      <img class="logo-figure" src="data/logos/calblue-logo.jpg" alt="Calblue Soccer Team"></a>
    </td>
  </tr>
</table>
<!-- <table>
  <tr>
<td class="logo-col2">
      <a href="https://www.amazon.science/research-areas/robotics" target="_blank" title="Amazon Robotics">
        <img class="logo-figure1" src="data/logos/amazon-robotics.png" alt="Amazon Robotics"></a>
    </td>
    <td class="logo-col2"></td>
    <td class="logo-col2"></td>
  </table> -->
<table>
  
  <tr>
    <td class="logo-col2">
      <a href="https://www.amazon.science/research-areas/robotics" target="_blank" title="Amazon Robotics">
        <img class="logo-figure1" src="data/logos/amazon-robotics.png" alt="Amazon Robotics"></a>
    </td>
    <td class="logo-col1">
      <a href="https://www.microsoft.com/en-us/research/" target="_blank" title="Microsoft Research">
        <img class="logo-figure1" src="data/logos/microsoft-logo.jpg" alt="Microsoft Research"></a>
    </td>
    <!-- <td class="logo-col1">
      <a href="https://mitibmwatsonailab.mit.edu/" target="_blank" title="MIT-IBM Watson AI Lab">
        <img class="logo-figure1" src="data/logos/mit-ibm-ai-lab-logo.png" alt="MIT-IBM Watson AI Lab"></a>
    </td> -->
    <td class="logo-col1">
      <a href="https://www.google.com/" target="_blank" title="Google">
        <img class="logo-figure1" src="data/logos/google-logo.jpg" alt="Google"></a>
    </td>
  </tr>
</table>
  <table>
    <tr>
      <td class="logo-col1">
        <a href="https://mitibmwatsonailab.mit.edu/" target="_blank" title="MIT-IBM Watson AI Lab">
          <img class="logo-figure1" src="data/logos/mit-ibm-ai-lab-logo.png" alt="MIT-IBM Watson AI Lab"></a>
      </td>
      <!-- <td class="logo-col1">
        <a href="https://www.google.com/" target="_blank" title="Google">
          <img class="logo-figure1" src="data/logos/google-logo.jpg" alt="Google"></a>
      </td> -->
    <td class="logo-col1">
      <a href="https://usa.honda-ri.com/" target="_blank" title="Honda Research">
        <img class="logo-figure1" src="data/logos/hri-us-logo-2018.png" alt="Honda Research"></a>
    </td>
    <td class="logo-col1">
      <a href="https://www.tusimple.com/" target="_blank" title="TuSimple">
        <img class="logo-figure1" src="data/logos/TuSimple-Logo.jpg" alt="TuSimple"></a>
    </td>
    <!-- <td class="logo-col1">
      <a href="https://smart.mit.edu/" target="_blank" title="Singapore-MIT Alliance">
        <img class="logo-figure1" src="data/logos/smart-logo.png" alt="Singapore-MIT Alliance"></a>
    </td> -->
  </tr>
  </table>


  <h2>Contact</h2>
<ul><li><p>Email: mengyuethu@gmail.com</p></li></ul>
<ul><li><p>Email: mengyue@mit.edu</p></li></ul>

<h2 id="publications_anchor">Publications</h2>
<table>
  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/audere_arch.png"/> </td>
    <td class="pub-text">
    <h3>AuDeRe: Automated Strategy Decision and Realization in Robot Planning and Control via LLMs</h3>
    <p><b>Yue Meng<sup>*</sup></b>, Fei Chen<sup>*</sup>, Yongchao Chen, Chuchu Fan<br />
    <i>Under review</i><br />
    [<a href="https://arxiv.org/pdf/2504.03015">arXiv</a>] [<a href="https://github.com/mengyuest/llm-planning-control">Code</a>]
    <p> We propose a framework that leverages LLMs to select appropriate planning and control strategies based on task descriptions, environmental constraints, and system dynamics. These strategies are then executed by calling the available comprehensive planning and control APIs. The results demonstrate that using LLMs to determine planning and control strategies from natural language descriptions significantly enhances robotic autonomy while reducing the need for extensive manual tuning and expert knowledge.</p>
    </td>
  </tr>
  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/telograf_arch.png"/> </td>
    <td class="pub-text">
    <h3>TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</h3>
    <p><b>Yue Meng</b>, Chuchu Fan<br />
    <i>Int. Conf. on Machine Learning (ICML), 2025</i><br />
    [<a href="https://arxiv.org/pdf/2505.00562">arXiv</a>][<a href="https://github.com/mengyuest/TeLoGraF">Code</a>]
    <p> We design a learning-based framework that uses Graph Neural Networks (GNN) and flow matching to solve general Signal Temporal Logic (STL) tasks. We collect a large-scale dataset of 200K STL specifications paired with demonstrations and then train our model.
      We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. 
      <!-- our method generalizes across diverse robotic environments and significantly outperforms classical STL planners in both speed and satisfaction rate. -->
    </p>
    </td>
  </tr>
  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/icra2025_demo_v1.gif"/> </td>
    <td class="pub-text">
    <h3>Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders</h3>
    <p><b>Yue Meng<sup>1</sup></b>, Nathalie Majcherczyk<sup>2</sup>, Wenliang Liu<sup>2</sup>, Scott Kiesel<sup>2</sup>, Chuchu Fan<sup>1</sup>, Federico Pecora<sup>2</sup><br />
    <sup>1</sup>MIT  &nbsp; &nbsp; <sup>2</sup>Amazon Robotics<br />
    <i>IEEE Int. Conf. on Robotics and Automation (ICRA), 2025</i><br />
    [<a href="http://arxiv.org/pdf/2503.02954">PDF</a>] [<a href="https://github.com/mengyuest/gnn-vae-coord">page</a>]
    <p> We design Graph Neural Network Variational Autoencoders (GNN-VAE) to generate global schedules that efficiently coordinate multi-robot movements. We collect ground truth schedules through a Mixed-Integer Linear Program (MILP) solver and train GNN to encode these solutions in a latent space. Solutions are then decoded following the dual-branch framework. By construction, our GNN-VAE returns solutions that always respect the constraints. Our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. </p>
    </td>
  </tr>
  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/ral2024.png"/> </td>
    <td class="pub-text">
    <h3>Diverse Controllable Diffusion Policy with Signal Temporal Logic</h3>
    <p><b>Yue Meng</b>, Chuchu Fan<br />
    <i>IEEE Robotics and Automation Letters (RA-L), 2024</i><br />
    [<a href="https://ieeexplore.ieee.org/iel8/7083369/7339444/10638176.pdf">PDF</a>] [<a href="https://github.com/mengyuest/pSTL-diffusion-policy">code</a>]
    <p> We leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the secondbest approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. </p>
    </td>
  </tr>
  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/icra2024_conbat.png"/> </td>
    <td class="pub-text">
    <h3>ConBaT: Control Barrier Transformer for Safe Policy Learning</h3>
    <p><b>Yue Meng<sup>1</sup></b>, Sai Vemprala<sup>2</sup>, Rogerio Bonatti<sup>3</sup>, Chuchu Fan<sup>1</sup>, Ashish Kapoor<sup>2</sup><br />
    <sup>1</sup>MIT  &nbsp; &nbsp; <sup>2</sup>Scaled Foundations  &nbsp; &nbsp; <sup>3</sup>Microsoft Research  <br />
    <i>IEEE Int. Conf. on Robotics and Automation (ICRA), 2024</i><br />
    [<a href="https://arxiv.org/pdf/2303.04212.pdf">arXiv</a>] 
    <p> We propose Control Barrier Transformer (ConBaT), an approach to learn safe behaviors from demonstrations. ConBaT is inspired by control barrier functions and uses a causal transformer to predict safe actions with a critic that requires minimal labeling. During deployment, we employ a lightweight optimization to ensure future states lie within the learned safe set. We apply our approach to control tasks and show that ConBaT results in safer policies compared to classical and learning-based baselines such as imitation learning, reinforcement learning, and model predictive control.
  </p>
    </td>
  </tr>
  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/ral2023.gif"/> </td>
    <td class="pub-text">
    <h3>Signal Temporal Logic Neural Predictive Control</h3>
    <p><b>Yue Meng</b>, Chuchu Fan<br />
    <i>IEEE Robotics and Automation Letters (RA-L), 2023</i><br />
    [<a href="https://arxiv.org/pdf/2309.05131.pdf">arXiv</a>] [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10251585">paper</a>] [<a href="https://github.com/mengyuest/stl_npc">code</a>][<a href="https://mit-realm.github.io/stl_mpc/">page</a>] [<a href="https://youtu.be/Bc0OaGydCB0">video</a>] 
    <p> We learn a neural network controller to satisfy the requirements specified in Signal Temporal Logic (STL). Our controller learns to maximize the STL robustness score in training. In testing, our controller predicts within a planning horizon to satisfy STL. A backup policy is designed to ensure safety when our controller fails. On six tasks we outperform classical methods (MPC, STL-solver), model-free and model-based RL methods in STL satisfaction rate, especially on tasks with complex STL, while being 10X-100X faster than the classical methods.
    </p>
    </td>
  </tr>
  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/l4dc2023.gif"/> </td>
    <td class="pub-text">
    <h3>Hybrid Systems Neural Control with Region-of-Attraction Planner</h3>
    <p><b>Yue Meng</b>, Chuchu Fan<br />
    <i>5th Annual Learning for Dynamics & Control Conference (L4DC), 2023</i><br />
    [<a href="https://arxiv.org/pdf/2303.10327.pdf">arXiv</a>] [<a href="https://proceedings.mlr.press/v211/meng23a/meng23a.pdf">paper</a>] [<a href="https://github.com/mengyuest/nn_roa_planner">code</a>][<a href="https://mit-realm.github.io/hybrid-clf/">page</a>] [<a href="data/poster_l4dc2023_final.pdf">poster</a>] 
    <p>In this work, we propose a neural network (NN)-based method to control hybrid systems. Upon mode switching, we propose a differentiable planner to ensure the states after switching can land in next mode's RoA, hence stabilizing the hybrid system. With low running time, our controller achieves a higher stability/success rate over MPC, RL, common Lyapunov methods (CLF), linear quadratic regulator (LQR), quadratic programming (QP) and Hamilton-Jacobian-based methods (HJB).
    </p>
    </td>
  </tr>
  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/icra2023.gif"/> </td>
    <td class="pub-text">
    <h3>Density Planner: Minimizing Collision Risk in Motion Planning with Dynamic Obstacles using Density-based Reachability</h3>
    <p>Laura Lützow, <b>Yue Meng</b>, Andres Chavez Armijos, Chuchu Fan<br />
    <i>IEEE Int. Conf. on Robotics and Automation (ICRA), 2023</i><br />
    [<a href="https://arxiv.org/pdf/2210.02131.pdf">arXiv</a>] [<a href="https://youtu.be/jdwnddtrBnI">video</a>] [<a href="https://github.com/MIT-REALM/density_planner">code</a>]
    <p>We propose a density-based method which uses a neural network and the Liouville equation to learn the density evolution for a system with an uncertain initial state. We conduct motion planning experiments on simulated environments and environments generated from real-world data and outperform baseline methods such as model predictive control and nonlinear programming. While our method requires offline planning, the online run speed is 100times faster compared to model predictive control. 
    </p>
    </td>
  </tr>

  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/nfm2022.png"/> </td>
    <td class="pub-text">
    <h3>Density of Reachable States and How to Use it for Safe Autonomous Motion Planning</h3>
    <p><b>Yue Meng</b>, Zeng Qiu, Md Tawhid Bin Waez, Chuchu Fan<br />
    <i>NASA Formal Methods Symposium (NFM), 2022</i><br />
    [<a href="https://drive.google.com/file/d/1Q-cU3MRY-nd2wydaSzwJ76cMy9DVIjQa/view?usp=sharing">pdf</a>]
    <p>
     Recent work provides a data-driven approach to compute the density distribution of autonomous systems’ forward reachable states online. In this paper, we study the use of such approach in combination with model predictive control for verifiable safe path planning under uncertainties. We design two challenging scenarios (autonomous driving and hovercraft control) for safe motion planning in environments with obstacles under system uncertainties. By leveraging the estimated risk, our algorithm achieves the highest success rate in goal reaching when enforcing the safety rate above 0.99.</p>
    </td>
  </tr>

  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/corl2021.png"/> </td>
    <td class="pub-text">
    <h3>Learning Density Distribution of Reachable States for Autonomous Systems</h3>
    <p><b>Yue Meng</b>, Dawei Sun, Zeng Qiu, Md Tawhid Bin Waez, Chuchu Fan<br />
    <i>Conference of Robotics and Learning (CoRL), 2021</i><br />
    [<a href="https://arxiv.org/pdf/2109.06728.pdf">arXiv</a>][<a href="https://github.com/mengyuest/nn_reachable_density/">code</a>][<a href="https://youtu.be/GiL7pRFgr0s">seminar</a>]
    <p>In this work, we propose a data-driven method to compute the density distribution of reachable states for nonlinear systems. Our approach learns system dynamics and the state density jointly from trajectory data, guided by the fact that the state density evolution follows the Liouville partial differential equation. We show that our learned solution can produce a much more accurate estimate on density distribution, and can quantify risks less conservatively and flexibly compared with worst-case analysis.</p>
    </td>
  </tr>

  <tr>
    <!-- <td class="pub-figure"> <img class="pub-img" src="data/preprint.png"/> </td> -->
    <td class="pub-figure"> <img class="pub-img" src="data/iros2021.gif"/> </td>
    <td class="pub-text">
      <h3>Reactive and Safe Road User Simulations using Neural Barrier Certificates</h3>
      <p><b>Yue Meng</b>, Zengyi Qin, Chuchu Fan<br />
      <i>IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2021.</i> <br />
      [<a href="https://arxiv.org/pdf/2109.06689.pdf">arXiv</a>][<a href="http://realm.mit.edu/reactive-agent-models/">page</a>][<a href="https://github.com/mengyuest/reactive_cbf/">code</a>][<a href="https://youtu.be/GiL7pRFgr0s">seminar</a>]
      <p>We proposed a reactive agent model which can ensure safety by learning high-level decisions from expert data and a low level decentralized controller guided by the jointly learned decentralized barrier certificates. Empirical results show that our approach can achieve a significant improvement in safety while being similar to human behaviors.</p>
    </td>
  </tr>

  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/iclr2021.gif"/> </td>
    <td class="pub-text">
      <h3>AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition</h3>
      <p><b>Yue Meng</b>, Rameswar Panda, Chung-Ching Lin, Prasanna Sattigeri, Leonid Karlinsky, Kate Saenko, Aude Oliva, Rogerio Feris<br />
      <i>Int. Conf. on Learning Representations (ICLR), 2021.</i><br />
    [<a href="https://arxiv.org/pdf/2102.05775.pdf">arXiv</a>][<a href="https://mengyuest.github.io/AdaFuse/">page</a>][<a href="https://github.com/mengyuest/AdaFuse">code</a>]</p> 
      <p>This paper introduces an adaptive temporal fusion, AdaFuse, that dynamically fuses channels for strong temporal modelling. Extensive experiments on SomethingV1\&V2, Jester and Mini-Kinetics show AdaFuse achieves 40\% computation savings with comparable accuracy to state-of-the-art methods</p>
    </td>
  </tr>

  <tr>
    <td class="pub-figure"> <img class="pub-img" src="data/eccv2020.gif"/> </td>
    <td class="pub-text">
      <h3>AR-Net: Adaptive Frame Resolution for Efficient Action Recognition</h3>
      <p><b>Yue Meng</b>, Chung-Ching Lin, Rameswar Panda, Prasanna Sattigeri, Leonid Karlinsky, Aude Oliva, Kate Saenko, Rogerio Feris<br />
      <i>European Conf. on Computer Vision (ECCV), 2020.</i><br />
    [<a href="https://arxiv.org/pdf/2007.15796.pdf">arXiv</a>][<a href="https://mengyuest.github.io/AR-Net">page</a>][<a href="https://github.com/mengyuest/AR-Net">code</a>]</p>  
    <p>This paper proposes a novel approach, AR-Net (Adaptive  Resolution  Network),  that  selects  on-the-fly the optimal resolution for each frame conditioned on the input for efficient action recognition in long untrimmed videos. Extensive experiments on several action recognition datasets well demonstrate the efficacy of our method over state-of-the-art.</p>
    </td>
  </tr>

<tr>
  <td class="pub-figure"> <img class="pub-img" src="data/icra2020_after.gif"/> </td>
  <td class="pub-text">
    <h3>Learning 3D-aware Egocentric Spatial-Temporal Interaction via Graph Convolutional Networks</h3>
    <p>Chengxi Li, <b>Yue Meng</b>, Stanley H. Chan and Yi-Ting Chen<br />
    <i>IEEE Int. Conf. on Robotics and Automation (ICRA), 2020.</i><br />
    [<a href="https://arxiv.org/pdf/1909.09272.pdf">arXiv</a>][<a href="https://sites.google.com/site/yitingchen0524/research/driver-centric-risk-assessment/">page</a>]</p>
    <p>This paper proposes a 3D-aware egocentric spatial-temporalinteraction framework for automated   driving applications. Graph convolution networks (GCN)  is  devised  for  interaction modeling. Extensive   experiments   are   conducted using  Honda  Research  Institute  Driving  Dataset.</p>
  </td>
</tr>
  
<tr>
  <td class="pub-figure"> <img class="pub-img" src="data/iros2019.gif"/> </td>
  <td class="pub-text">
    <h3>Localization and Mapping using Instance-specific Mesh Models</h3>
    <p>Qiaojun Feng, <b>Yue Meng</b>, Mo Shan and Nikolay Atanasov<br />
    <i>IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2019.</i> <br />
    [<a href=" https://natanaso.github.io/ref/Feng_DeformableMeshModel_IROS19.pdf">pdf</a>]</p>
    <p>This paper focuses on building semantic maps, containing object poses and shapes, using a monocular camera. Our
    contribution is an instance-specific mesh model of object shape
    that can be optimized online based on semantic information extracted from camera images.</p>
    </td>
</tr>

<tr>
  <td class="pub-figure"> <img class="pub-img" src="data/signet.gif"/> </td>
  <td class="pub-text">
    <h3>SIGNet: Semantic Instance Aided Unsupervised 3D Geometry Perception</h3>
    <p><b>Yue Meng</b>, Yongxi Lu, Aman Raj, Samuel Sunarjo, Rui Guo, Tara Javidi, Gaurav Bansal, Dinesh Bharadia<br />
    <i>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019.</i> <br />
    [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Meng_SIGNet_Semantic_Instance_Aided_Unsupervised_3D_Geometry_Perception_CVPR_2019_paper.pdf">pdf</a>][<a href="https://mengyuest.github.io/SIGNet/">page</a>][<a href="https://github.com/mengyuest/SIGNet">code</a>]</p>
    <p>This paper introduces SIGNet, a novel framework that provides robust geometry perception without requiring geometrically informative labels. SIGNet is shown to improve upon the state of art unsupervised learning for geometry perception by 30%</p>
  </td>
</tr>

<tr>
  <td class="pub-figure"> <img class="pub-img" src="data/rss2018workshop-1.png"/> </td>
  <td class="pub-text">
    <h3>Dense Spatial Segmentation from Sparse Semantic Information</h3>
    <p>Qiaojun Feng, <b>Yue Meng</b> and Nikolay Atanasov<br />
    <i>Workshop at Robotics: Science and Systems (RSS), 2018.</i> <br />
    [<a href="https://drive.google.com/file/d/1b1C4SMfS0aPwHM7mT4TSl59rD6pnD6gY/view">pdf</a>]</p>
    <p>This paper develops an environment representation that affords reasoning about the occupancy of space, necessary for safe navigation, and about the identity of objects, necessary for complex task interpretation.</p>
  </td>
</tr>

<tr>
  <td class="pub-figure"> <img class="pub-img" src="data/cosim.gif"/> </td>
  <td class="pub-text">
    <h3>Cooperative Driving Strategies for Nonsignalized Intersections</h3>
    <p><b>Yue Meng</b>, Li Li, Fei-Yue Wang, Keqiang Li and Zhiheng Li<br />
    <i>IEEE Transactions on Vehicular Technology (TVT), 2017.</i> <br />
    [<a href="https://www.researchgate.net/profile/Li_Li240/publication/321640653_Analysis_of_Cooperative_Driving_Strategies_for_Non-Signalized_Intersections/links/5a2a88e245851552ae77e917/Analysis-of-Cooperative-Driving-Strategies-for-Non-Signalized-Intersections.pdf">pdf</a>]</p>
    <p>We study the difference between two major strategies of cooperative driving at intersections: the “ad hoc negotiation based” strategy and the "planning based" strategy. We carry out a series of simulations under different traffic scenarios for comparison. Results indicate the performance of a strategy mainly depends on the passing order of vehicles that it finds.</p>
  </td>
</tr>

</table>

<!-- <div>
  <span class="pub-figure"> <img class="pub-img" src="data/signet.gif"/> </span>
  <span class="pub-text"></span>
</div> -->



<h2>Links</h2>
  <ul>
  <li><p><a href="https://existentialrobotics.org">Existential Robotics Laboratory</a></p>
  </li>
  <li><p><a href="https://wcsng.ucsd.edu">Wireless Communication Systems and Networking Group</a></p>
  </li>
  <li><p><a href="https://www.bilibili.com/video/av35787189">自35班毕业纪念视频</a></p>
  </li>
  <li><p><a href="http://www.tsinghuaveterans.com/">Tsinghua Veteran Soccer Team</a></p>
  </li>
  <li><p><a href="https://sites.google.com/view/nccsfweb/teams/tsinghua-veterans-bay-area">Tsinghua Veteran Bay Area Soccer Team</a></p>
  </li>
  <li><p><a href="https://sites.google.com/view/nccsfweb/teams/calblue">CalBlue Soccer Team</a></p>
  </li>
  <li><p><a href="https://sites.google.com/view/sdred/">San Diego Red Army Soccer Team</a></p>
  </li>
  </ul>

<h2 id="collecting_anchor" class="collapsible">Collecting (click to see more)</h2>
<div class="content">
  <h3>Routine</h3>
  <ul>
  <li><p><a href="https://arxiv.org/list/cs.CV/recent">arXiv: Computer Vision and Pattern Recognition</a></p>
  </li>
  <li><p><a href="http://www.arxiv-sanity.com/">arXiv Sanity</a></p>
  </li>
  <li><p><a href="https://assert.pub/">Top 10 arXiv papers Today</a></p>
  </li>
  </ul>
  <h3>Procastination</h3>
  <ul>
  <li><p><a href="https://www.bilibili.com/video/av3584431/">央视纪录片《高考》</a></p>
  </li>
  <li><p><a href="https://aideadlin.es/?sub=RO,CV,ML">AI Conference Deadlines</a></p>
  </li>
  <li><p><a href="https://www.cmu.edu/randyslecture">Randy Pausch's Last Lecture</a></p>
  </li>
  <li><p><a href="https://www.youtube.com/watch?v=a1zDuOPkMSw">You and Your Research</a></p>
  </li>
  </ul>
  <h3>Ph.D.</h3>
  <ul>
  <li><p><a href="https://zhuanlan.zhihu.com/p/29636455">北美博士必读清单</a></p>
  </li>
  <li><p><a href="https://www.zhihu.com/question/28920341">大牛写的非技术类作品</a></p>
  </li>
  <li><p><a href="http://faculty.neu.edu.cn/cc/zhangyf/papers/CraftingFuture.pdf">Crafting Your Research Future</a></p>
  </li>
  <li><p><a href="https://www.cs.unc.edu/~azuma/hitch4.html">CS Graduate Survival Guide</a></p>
  </li>
  <li><p><a href="http://3dvision.princeton.edu/courses/COS598/2014sp/slides/lecture21_how2research.pdf">Doing research in Computer Vision</a></p>
  </li>
  </ul>
  <h3>Robotics</h3>
  <ul>
  <li><p><a href="https://github.com/kanster/awesome-slam">Awesome SLAM</a></p>
  </li>
  <li><p><a href="https://arxiv.org/pdf/1606.05830.pdf">Past, Present and Future of SLAM</a></p>
  </li>
  <li><p><a href="http://ais.informatik.uni-freiburg.de/teaching/ws11/robotics2/pdfs/rob2-13-frontends.pdf">SLAM Front-Ends</a></p>
  </li>
  <li><p><a href="http://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Triggs00.pdf">Bundle Adjustment: A Modern Synthesis</a></p>
  </li>
  <li><p><a href="http://cs.brown.edu/research/ai/pomdp/tutorial/">POMDP</a></p>
  </li>
  </ul>
  <h3>Technician</h3>
  <ul>
  <li><p><a href="http://www.docs.is.ed.ac.uk/skills/documents/3722/3722-2014.pdf">LaTeX for Beginners</a></p>
  </li>
  <li><p><a href="https://github.com/onqtam/awesome-cmake">Awesome CMake</a></p>
  </li>
  <li><p><a href="https://cgold.readthedocs.io/en/latest/">CGold: The Hitchhiker's Guide to the CMake</a></p>
  </li>
  </ul>
  <h3>Blogs</h3>
  <ul>
  <li><p><a href="http://www.computervisionblog.com/">Tombone's Computer Vision Blog</a></p>
  </li>
  <li><p><a href="http://www.pgbovine.net/writings.htm">Philip Guo</a></p>
  </li>
  <li><p><a href="https://www.kennethreitz.org/">Kenneth Reitz</a></p>
  </li>
  </ul>
</div>

  <h2 id="soccer_anchor" class="collapsible">Soccer (click to see more)</h2>
  <div class="content">
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-000.jpg" alt="soccer-000" width="500px" />&nbsp;</td>
  <td><p>Champion of Haidian District Soccer League, 2011</p></td></tr>
  <tr>
    <td><img class="misc-img" src="data/soccer-00.jpg" alt="soccer-00" width="500px" />&nbsp;</td>
    <td><p>High School Class Soccer Team, 2012</p></td>
  </tr>
  <tr><td>
  <img class="misc-img" src="data/soccer-da-0.png" alt="soccer-2" width="500px" />&nbsp;</td>
  <td><p>Dept. Automation Men's Soccer, 2013</p>
  </td></tr></table> -->
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-0.jpg" alt="soccer-0" width="500px" />&nbsp;</td>
  <td><p>High school friends in THU-PKU Derby, 2014 </p>
  </td></tr></table>
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-02.JPG" alt="soccer-02" width="500px" />&nbsp;</td>
  <td><p>Best class soccer team at Tsinghua, 2014 </p>
  </td></tr></table> -->
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-01.JPG" alt="soccer-01" width="500px" />&nbsp;</td>
  <td><p>Champion at Tsinghua Futsal Tournament, 2014 </p>
  </td></tr></table>
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-3.jpg" alt="soccer-3" width="500px" />&nbsp;</td>
  <td><p>Classmates in the soccer team, 2015</p>
  </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-4.png" alt="soccer-4" width="500px" />&nbsp;</td>
  <td><p>The 3rd place at Tsinghua Futsal Tournament, 2015</p>
  </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-9.jpg" alt="soccer-9" width="500px" />&nbsp;</td>
  <td><p>Coaching Dept. Automation Women's Soccer, 2015</p>
  </td></tr></table> -->
  <table class="imgtable"><tr><td>
    <img class="misc-img" src="data/soccer-13.png" alt="soccer-1" width="500px" />&nbsp;</td>
    <td><p>Champion at Automation Tournament, 2016 </p>
    </td></tr></table>
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-5.jpg" alt="soccer-5" width="500px" />&nbsp;</td>
  <td><p>Tsinghua Men's Soccer Team, 2016</p>
  </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-6.jpg" alt="soccer-6" width="500px" />&nbsp;</td>
  <td><p>Coaching Tsinghua Women's Soccer, 2016</p>
  </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-1.png" alt="soccer-1" width="500px" />&nbsp;</td>
  <td><p>Champion at Automation Tournament, 2017 </p>
  </td></tr></table> -->
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-7.jpg" alt="soccer-7" width="500px" />&nbsp;</td>
  <td><p>Champion at Chinese Alumni League, 2017</p>
  </td></tr></table>
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-8.jpg" alt="soccer-8" width="500px" />&nbsp;</td>
  <td><p>The 2nd place at Triton World Cup, 2017</p>
  </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
    <img class="misc-img" src="data/soccer-12.jpg" alt="soccer-12" width="500px" />&nbsp;</td>
    <td><p>NJ Star Soccer Team, Maryland, 2018</p>
    </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-10.jpg" alt="soccer-9" width="500px" />&nbsp;</td>
  <td><p>Tsinghua Veteran Bay Area Soccer Team, 2019</p>
  </td></tr></table> -->
  <table class="imgtable"><tr><td>
  <img class="misc-img"  src="data/soccer-10-.jpg" alt="soccer-10" width="500px" />&nbsp;</td>
  <td><p>Champion at SV Alumni Soccer Tournament, 2019</p>
  </td></tr></table>
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/soccer-11.jpg" alt="soccer-11" width="500px" />&nbsp;</td>
  <td><p>Calblue Soccer Team, 2019</p>
  </td></tr></table> -->
  </div>

  <h2 class="collapsible" id="running_anchor">Running (click to see more)</h2>
  <div class="content">
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-3.jpg" alt="running-4" width="500px" />&nbsp;</td>
  <td><p>High school record, 2012 (3000m race)</p>
  </td></tr></table>
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-4.jpg" alt="running-3" width="500px" />&nbsp;</td>
  <td><p>Unlimited relay race champion at BUAA, 2014 </p>
  </td></tr></table>
  <!-- <table class="imgtable"><tr><td>
    <img class="misc-img" src="data/run-ma.jpg" alt="running-ma" width="500px" />&nbsp;</td>
    <td><p>Dept. Automation, Champion of the 57th Tsinghua Sports Meeting, 2014</p>
    </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-6.jpg" alt="running-6" width="500px" />&nbsp;</td>
  <td><p>Best 4*800m race teams, Tsinghua Sports Meeting, 2014</p>
  </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-7.jpg" alt="running-7" width="500px" />&nbsp;</td>
  <td><p>10*1000m race champion, Tsinghua Sports Meeting, 2015</p>
  </td></tr></table> -->
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-2.png" alt="running-2" width="500px" />&nbsp;</td>
  <td><p>1500m race champion, Tsinghua Sports Meeting, 2015</p>
  </td></tr></table>
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-1.jpg" alt="running-1" width="500px" />&nbsp;</td>
  <td><p>4*800m race champion, Tsinghua Sports Meeting, 2015</p>
  </td></tr></table>
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-5.jpg" alt="running-5" width="500px" />&nbsp;</td>
  <td><p>4*400m race champion, Tsinghua Sports Meeting, 2016</p>
  </td></tr></table>
</div>
  <!-- <table class="imgtable"><tr><td>
    <img class="misc-img" src="data/run-11.jpg" alt="running-10" width="500px" />&nbsp;</td>
    <td><p>5000m 2nd place, 17th Northern California Chinese Culture Federation Sports Meeting, 2019</p>
    </td></tr></table> -->
  <!-- <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-9.jpg" alt="running-9" width="500px" />&nbsp;</td>
  <td><p>5000m Champion in Tsinghua Alumni Athletic Meeting (TAAM), 2019</p>
  </td></tr></table>
  <table class="imgtable"><tr><td>
  <img class="misc-img" src="data/run-10.jpg" alt="running-10" width="500px" />&nbsp;</td>
  <td><p>Dept. Automation won the Champion in the 1st TAAM, 2019</p>
  </td></tr></table> -->



  <footer> <!-- 本站所有网页的统一页脚 -->
    <!-- Statcounter code for My homepage https://mengyuest.github.io/ on Google Sites (new) -->
<script type="text/javascript">
  var sc_project=11797803;
  var sc_invisible=0;
  var sc_security="54080103";
  var sc_text=2;
  var scJsHost = "https://";
  document.write("浏览量: <sc"+"ript type='text/javascript' src='" +
  scJsHost+
  "statcounter.com/counter/counter.js'></"+"script>");
  </script>
  <noscript><div class="statcounter"><a title="hit counter"
  href="https://statcounter.com/" target="_blank"><img
  class="statcounter"
  src="https://c.statcounter.com/11797803/0/54080103/0/"
  alt="hit counter"></a></div></noscript>
  <!-- End of Statcounter Code -->
  </footer>

  <script>
    var coll = document.getElementsByClassName("collapsible");
    console.log(coll.length);
    var i;
  
    for (i = 0; i < coll.length; i++) {
      // coll[i].addEventListener("click", function() {
      //   this.classList.toggle("active");
      //   var content = this.nextElementSibling;
      //   if (content.style.display === "block") {
      //     content.style.display = "none";
      //   } else {
      //     content.style.display = "block";
      //   }
      // });

      coll[i].onclick = function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
          this.textContent = this.textContent.split(" ")[0] + " (click to see more)";
        } else {
          content.style.display = "block";
          this.textContent = this.textContent.split(" ")[0] + " (click to collapse)";
        }
      };
    }
  </script>

  <script>
    function collapseFunction(id) {
      var coll = document.getElementById(id);
      coll.classList.toggle("active");
      var content = coll.nextElementSibling;
      content.style.display = "block";
      coll.textContent = coll.textContent.split(" ")[0] + " (click to collapse)";
    }
  </script>

</body>
</html>
